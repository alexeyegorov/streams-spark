{
  "name": "Streams-spark",
  "tagline": "",
  "body": "# Streams Spark #\r\n\r\nThis project is based upon [streams-flink](https://bitbucket.org/AEgorov/streams-flink) that tries to slightly modify [streams-storm](https://bitbucket.org/cbockermann/streams-storm) project in order to adapt it to ``Flink``.\r\nHere we do the same procedure to adapt it to ``Spark Streaming``.\r\nThis way we can achieve parsing of XML configuration files for ``streams framework`` and translating them into ``Spark``'s data flow graph.\r\n\r\n\r\nThe XML definition of ``streams`` process has not been changed.\r\nWe have support for ``services`` and ``queues``. \r\nWe can still use ``copies`` attribute in ``<process ...>`` tag in order to controll the level of parallelism.\r\nEach copy is then mapped to a task slot inside of the Flink cluster.\r\nEach ``process``, e.g. as the following\r\n```\r\n<process input=\"data\" id=\"extraction\" copies=\"1\">\r\n```\r\n\r\nis translated to a flatMap function.\r\n\r\n```\r\nJavaDStream<Data> dataJavaDStream = receiver.flatMap(function);\r\n```\r\nParallelism level is set over ``copies`` attribute.\r\nIn Spark Streaming it can be defined through the number of the partitions in a RDD.\r\nOn the one hand it can be set through ``batch interval / block interval`` or we can repartition the data manually as followed:\r\n\r\n```\r\nif (el.hasAttribute(\"copies\")) {\r\n\treceiver = receiver.repartition(Integer.parseInt(element.getAttribute(\"copies\")));\r\n}\r\n```\r\n\r\nThis value should be computed as the number of all available cores minus 1 core for the driver.\r\n\r\nThe easiest way to start a Spark job is to use the submit script by Spark itself (many configurations can be set here or directly in code):\r\n\r\n```\r\n./bin/spark-submit --class spark.deploy_on_spark --master spark://129.217.30.197:6066 \\\r\n            --deploy-mode cluster \\\r\n            --executor-memory 9G \\\r\n            --driver-memory 2G \\\r\n            --num-executors 2 --executor-cores 5 \\\r\n            --conf \"spark.eventLog.dir=file:///home/egorov/spark-eventlog\" \\\r\n            --conf \"spark.eventLog.enabled=true\" \\\r\n            --conf \"spark.streaming.unpersist=true\" \\\r\n            --conf \"spark.ui.showConsoleProgress=false\" \\\r\n            --conf \"spark.streaming.backpressure.enabled=true\" \\\r\n            --conf \"spark.streaming.ui.retainedBatches=300\" \\\r\n            --conf \"spark.ui.retainedStages=300\" \\\r\n            --conf \"spark.locality.wait=1s\" \\\r\n            --conf \"spark.locality.wait.node=0s\" \\\r\n            --conf \"spark.locality.wait.rack=1s\" \\\r\n            --conf \"spark.worker.cleanup.enabled=true\" \\\r\n            --conf \"spark.worker.cleanup.interval=1800\" \\\r\n            --conf \"spark.executor.logs.rolling.maxRetainedFiles=4\" \\\r\n            --conf \"spark.executor.logs.rolling.strategy=time\" \\\r\n            --conf \"spark.executor.logs.rolling.time.interval=hourly\" \\\r\n            --conf \"spark.executor.extraJavaOptions=-XX:MaxPermSize=2G -XX:+UseConcMarkSweepGC -Dlog4j.configuration=log4j-eir.properties\" \\\r\n            file:///home/egorov/streams-spark-0.9.25-SNAPSHOT-spark-provided.jar /home/egorov/example.xml\r\n```\r\n\r\n``spark.locality.wait{.node,.rack}`` setting can be used to enable splitting the tasks among several nodes.\r\nOtherwise default setting of 3 seconds will be used and possibly all tasks will run on the node where the data is received.\r\n\r\nSome further optimizations that are mentioned above has been described [here](https://blog.cloudera.com/blog/2016/01/how-cigna-tuned-its-spark-streaming-app-for-real-time-processing-with-apache-kafka/).",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}